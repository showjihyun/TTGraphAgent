"""
Advanced NLP Processor for Knowledge Graph Construction
5-Stage Pipeline: Preprocessing â†’ NER â†’ Relation Extraction â†’ Attribute Parsing â†’ LLM Validation
"""

import re
import json
import logging
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass
from collections import defaultdict

# NLP Libraries
import spacy
import nltk
from konlpy.tag import Okt, Mecab
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
except:
    pass

logger = logging.getLogger(__name__)

@dataclass
class Entity:
    """ì—”í‹°í‹° í´ë˜ìŠ¤"""
    id: str
    text: str
    label: str
    start: int
    end: int
    confidence: float
    attributes: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.attributes is None:
            self.attributes = {}

@dataclass
class Relation:
    """ê´€ê³„ í´ë˜ìŠ¤"""
    source: str
    target: str
    relation: str
    confidence: float
    context: str
    attributes: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.attributes is None:
            self.attributes = {}

class AdvancedNLPProcessor:
    """ê³ ê¸‰ NLP ì²˜ë¦¬ê¸°"""
    
    def __init__(self, language: str = "korean"):
        self.language = language
        self.device_info = self._get_device_info()
        self.setup_models()
    
    def _get_device_info(self) -> Dict[str, Any]:
        """GPU/CPU ì¥ì¹˜ ì •ë³´ í™•ì¸"""
        try:
            import torch
            
            cuda_available = torch.cuda.is_available()
            device_info = {
                "torch_available": True,
                "cuda_available": cuda_available,
                "device_count": torch.cuda.device_count() if cuda_available else 0,
                "current_device": torch.cuda.current_device() if cuda_available else None,
                "device_name": torch.cuda.get_device_name(0) if cuda_available else "CPU",
                "cuda_version": torch.version.cuda if cuda_available else None,
                "pytorch_version": torch.__version__
            }
            
            if cuda_available:
                device_info["gpu_memory"] = {
                    "total": torch.cuda.get_device_properties(0).total_memory,
                    "allocated": torch.cuda.memory_allocated(0),
                    "cached": torch.cuda.memory_reserved(0)
                }
                logger.info(f"ğŸš€ GPU DETECTED: {device_info['device_name']}")
                logger.info(f"GPU Memory: {device_info['gpu_memory']['total'] / 1024**3:.1f}GB total")
            else:
                logger.warning("âš ï¸  No GPU detected, will use CPU")
            
            logger.info(f"PyTorch version: {device_info['pytorch_version']}")
            return device_info
            
        except ImportError:
            logger.warning("PyTorch not available, using CPU only")
            return {"torch_available": False, "cuda_available": False}
        
    def setup_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # 1. SpaCy ëª¨ë¸ (ì˜ì–´ë§Œ)
            if self.language == "english":
                try:
                    self.spacy_nlp = spacy.load("en_core_web_sm")
                    logger.info("English SpaCy model loaded successfully")
                except OSError:
                    logger.warning("en_core_web_sm not found, using blank English model")
                    self.spacy_nlp = spacy.blank("en")
            else:
                # í•œêµ­ì–´ì˜ ê²½ìš° SpaCy ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í† í¬ë‚˜ì´ì € ì˜ì¡´ì„± ë¬¸ì œ ì™„ì „ íšŒí”¼)
                logger.info("Korean language: Skipping SpaCy to avoid tokenizer dependencies")
                self.spacy_nlp = None
            
            # 2. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ì„ íƒì )
            if self.language == "korean":
                try:
                    # Oktë§Œ ì‹œë„ (MeCab ì˜ì¡´ì„± ì™„ì „ íšŒí”¼)
                    from konlpy.tag import Okt
                    self.korean_analyzer = Okt()
                    logger.info("Okt Korean analyzer initialized successfully")
                except Exception as okt_error:
                    logger.warning(f"Korean analyzer initialization failed: {okt_error}")
                    logger.info("Will use rule-based processing only")
                    self.korean_analyzer = None
            else:
                self.korean_analyzer = None
            
            # 3. HuggingFace NER ëª¨ë¸ (ì„ íƒì )
            try:
                if self.language == "korean":
                    model_name = "klue/bert-base-korean-ner"
                else:
                    model_name = "dbmdz/bert-large-cased-finetuned-conll03-english"
                
                # ê°•í™”ëœ GPU ìš°ì„  ì‚¬ìš© ì„¤ì •
                import torch
                
                if torch.cuda.is_available():
                    device = 0  # GPU device 0
                    torch.cuda.set_device(0)
                    logger.info(f"NER will use GPU: {torch.cuda.get_device_name(0)}")
                else:
                    device = -1  # CPU
                    logger.warning("NER will use CPU")
                
                # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„ (GPU ìš°ì„ )
                self.ner_pipeline = pipeline(
                    "ner",
                    model=model_name,
                    tokenizer=model_name,
                    aggregation_strategy="simple",
                    device=device,
                    torch_dtype=torch.float16 if device == 0 else torch.float32  # GPUì—ì„œ float16 ì‚¬ìš©
                )
                device_name = "GPU" if device == 0 else "CPU"
                logger.info(f"NER pipeline initialized with {model_name} on {device_name}")
                    
            except Exception as e:
                logger.warning(f"NER pipeline failed to load: {e}")
                self.ner_pipeline = None
            
            # 4. ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ (ì„ íƒì )
            try:
                if self.language == "korean":
                    # ë” ê°€ë²¼ìš´ í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©
                    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                else:
                    model_name = "sentence-transformers/all-MiniLM-L6-v2"
                
                # ê°•í™”ëœ GPU ìš°ì„  ì‚¬ìš© ì„¤ì •
                import torch
                import os
                
                # CUDA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                
                # GPU ê°•ì œ ìš°ì„  ì„¤ì •
                if torch.cuda.is_available():
                    device = "cuda:0"
                    torch.cuda.set_device(0)
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()
                    logger.info(f"GPU detected: {torch.cuda.get_device_name(0)}")
                else:
                    device = "cpu"
                    logger.warning("No GPU available, using CPU")
                
                # SentenceTransformer ì´ˆê¸°í™” ì‹œ ëª…ì‹œì  device ì„¤ì •
                self.sentence_model = SentenceTransformer(model_name)
                self.sentence_model = self.sentence_model.to(device)
                
                # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ device í™•ì¸
                actual_device = next(self.sentence_model.parameters()).device
                logger.info(f"Sentence transformer initialized with {model_name}")
                logger.info(f"Target device: {device}, Actual device: {actual_device}")
                
            except Exception as e:
                logger.warning(f"Sentence transformer failed to load: {e}")
                self.sentence_model = None
                
        except Exception as e:
            logger.error(f"Model setup failed: {e}")
            self.setup_fallback_models()
    
    def setup_fallback_models(self):
        """í´ë°± ëª¨ë¸ ì„¤ì •"""
        self.spacy_nlp = None
        self.korean_analyzer = None
        self.ner_pipeline = None
        self.sentence_model = None
        logger.info("Using fallback rule-based processing only")
    
    def process_text(self, text: str) -> Dict[str, Any]:
        """5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            # 1ï¸âƒ£ ì „ì²˜ë¦¬
            preprocessed = self.preprocess_text(text)
            
            # 2ï¸âƒ£ ì—”í‹°í‹° ì¶”ì¶œ (NER)
            entities = self.extract_entities(preprocessed)
            
            # 3ï¸âƒ£ ê´€ê³„ ì¶”ì¶œ (RE)
            relations = self.extract_relations(preprocessed, entities)
            
            # 4ï¸âƒ£ ì†ì„± ì¶”ì¶œ
            entities_with_attributes = self.extract_attributes(preprocessed, entities)
            
            # 5ï¸âƒ£ êµ¬ì¡°í™” + ê²€ì¦ (LLM ë³´ì •ì€ ë³„ë„ í˜¸ì¶œ)
            structured_graph = self.structure_and_validate(
                entities_with_attributes, relations, preprocessed
            )
            
            return {
                "original_text": text,
                "preprocessed_text": preprocessed,
                "entities": [self._entity_to_dict(e) for e in entities_with_attributes],
                "relations": [self._relation_to_dict(r) for r in relations],
                "graph": structured_graph,
                "processing_stats": self._get_processing_stats(entities_with_attributes, relations)
            }
            
        except Exception as e:
            logger.error(f"Text processing failed: {e}")
            return self._fallback_processing(text)
    
    def preprocess_text(self, text: str) -> str:
        """1ï¸âƒ£ ì „ì²˜ë¦¬: ë¬¸ì¥ ë¶„ë¦¬ ë° ì •ì œ"""
        try:
            # ê¸°ë³¸ ì •ì œ
            text = re.sub(r'\s+', ' ', text)  # ë‹¤ì¤‘ ê³µë°± ì œê±°
            text = re.sub(r'[^\w\sê°€-í£.,!?;:()\-\[\]{}"]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            text = text.strip()
            
            # ë¬¸ì¥ ë¶„ë¦¬
            if self.language == "korean" and self.korean_analyzer:
                # í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
                sentences = self._split_korean_sentences(text)
            else:
                # ì˜ì–´ ë˜ëŠ” í´ë°± ë¬¸ì¥ ë¶„ë¦¬
                sentences = self._split_sentences_basic(text)
            
            # ë¬¸ì¥ ì •ì œ ë° ì¬ê²°í•©
            cleaned_sentences = []
            for sent in sentences:
                sent = sent.strip()
                if len(sent) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                    cleaned_sentences.append(sent)
            
            return ' '.join(cleaned_sentences)
            
        except Exception as e:
            logger.error(f"Preprocessing failed: {e}")
            return text
    
    def extract_entities(self, text: str) -> List[Entity]:
        """2ï¸âƒ£ ì—”í‹°í‹° ì¶”ì¶œ (NER)"""
        entities = []
        
        try:
            # HuggingFace NER ì‚¬ìš©
            if self.ner_pipeline:
                ner_results = self.ner_pipeline(text)
                for i, result in enumerate(ner_results):
                    entity = Entity(
                        id=f"entity_{i}",
                        text=result['word'],
                        label=result['entity_group'],
                        start=result['start'],
                        end=result['end'],
                        confidence=result['score']
                    )
                    entities.append(entity)
            
            # SpaCy NER ë³´ì™„
            if self.spacy_nlp and len(entities) < 3:
                doc = self.spacy_nlp(text)
                for i, ent in enumerate(doc.ents):
                    entity = Entity(
                        id=f"spacy_entity_{i}",
                        text=ent.text,
                        label=ent.label_,
                        start=ent.start_char,
                        end=ent.end_char,
                        confidence=0.8
                    )
                    entities.append(entity)
            
            # í´ë°±: ê·œì¹™ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
            if len(entities) < 2:
                entities.extend(self._extract_entities_rule_based(text))
            
            # ì¤‘ë³µ ì œê±° ë° ì •ì œ
            entities = self._deduplicate_entities(entities)
            
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            entities = self._extract_entities_rule_based(text)
        
        return entities
    
    def extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """3ï¸âƒ£ ê´€ê³„ ì¶”ì¶œ (RE)"""
        relations = []
        
        try:
            # íŒ¨í„´ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ í•˜ë‚˜ì˜ ë°©ë²•ë§Œ ì‚¬ìš©)
            pattern_relations = self._extract_relations_by_patterns(text, entities)
            relations.extend(pattern_relations)
            
            # ì„ë² ë”© ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ (ì„ íƒì , íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œë§Œ)
            if self.sentence_model and len(relations) == 0:
                embedding_relations = self._extract_relations_by_embeddings(text, entities)
                relations.extend(embedding_relations)
            
            # ê´€ê³„ ì¤‘ë³µ ì œê±°
            relations = self._deduplicate_relations(relations)
            
        except Exception as e:
            logger.error(f"Relation extraction failed: {e}")
            relations = self._extract_relations_fallback(entities)
        
        return relations
    
    def extract_attributes(self, text: str, entities: List[Entity]) -> List[Entity]:
        """4ï¸âƒ£ ì†ì„± ì¶”ì¶œ"""
        try:
            for entity in entities:
                # ë‚ ì§œ ì†ì„±
                entity.attributes.update(self._extract_date_attributes(text, entity))
                
                # ìˆ˜ì¹˜ ì†ì„±
                entity.attributes.update(self._extract_numeric_attributes(text, entity))
                
                # ìœ„ì¹˜ ì†ì„±
                entity.attributes.update(self._extract_location_attributes(text, entity))
                
                # ì¹´í…Œê³ ë¦¬ ì†ì„±
                entity.attributes.update(self._extract_category_attributes(text, entity))
                
        except Exception as e:
            logger.error(f"Attribute extraction failed: {e}")
        
        return entities
    
    def structure_and_validate(self, entities: List[Entity], relations: List[Relation], text: str) -> Dict[str, Any]:
        """5ï¸âƒ£ êµ¬ì¡°í™” + ê²€ì¦"""
        try:
            # ë…¸ë“œ ìƒì„± (í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì— ë§ê²Œ)
            nodes = []
            for entity in entities:
                node = {
                    "id": entity.text,  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                    "label": entity.text,
                    "group": entity.label,  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ group í•„ë“œ ì‚¬ìš©
                    "confidence": entity.confidence,
                    "attributes": entity.attributes,
                    "x": np.random.randint(100, 700),
                    "y": np.random.randint(100, 600)
                }
                nodes.append(node)
            
            # ì—£ì§€ ìƒì„± (í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì— ë§ê²Œ)
            edges = []
            
            for i, relation in enumerate(relations):
                edge = {
                    "id": f"edge_{i}",
                    "from": relation.source,  # ì´ë¯¸ í…ìŠ¤íŠ¸ë¡œ ì„¤ì •ë¨
                    "to": relation.target,    # ì´ë¯¸ í…ìŠ¤íŠ¸ë¡œ ì„¤ì •ë¨
                    "label": relation.relation,
                    "confidence": relation.confidence,
                    "context": relation.context,
                    "attributes": relation.attributes
                }
                edges.append(edge)
            
            # ê·¸ë˜í”„ í†µê³„
            summary = {
                "node_count": len(nodes),
                "edge_count": len(edges),
                "node_types": list(set([n["group"] for n in nodes])),  # group í•„ë“œ ì‚¬ìš©
                "processing_language": self.language,
                "confidence_avg": np.mean([n["confidence"] for n in nodes]) if nodes else 0
            }
            
            return {
                "nodes": nodes,
                "edges": edges,
                "summary": summary
            }
            
        except Exception as e:
            logger.error(f"Structuring failed: {e}")
            return {"nodes": [], "edges": [], "summary": {}}
    
    # Helper Methods
    def _split_korean_sentences(self, text: str) -> List[str]:
        """í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬"""
        sentences = re.split(r'[.!?]+\s*', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _split_sentences_basic(self, text: str) -> List[str]:
        """ê¸°ë³¸ ë¬¸ì¥ ë¶„ë¦¬"""
        try:
            if self.spacy_nlp:
                doc = self.spacy_nlp(text)
                return [sent.text.strip() for sent in doc.sents]
            else:
                return nltk.sent_tokenize(text)
        except:
            return re.split(r'[.!?]+\s*', text)
    
    def _extract_entities_rule_based(self, text: str) -> List[Entity]:
        """ê·œì¹™ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []
        
        try:
            # í•œêµ­ì–´ íŒ¨í„´ (ë” ì •í™•í•˜ê²Œ)
            if self.language == "korean":
                patterns = [
                    # ì¸ëª… íŒ¨í„´ (ëª…í™•í•œ í˜¸ì¹­ì´ ìˆëŠ” ê²½ìš°ë§Œ)
                    (r'[ê°€-í£]{2,4}(?:\s*(?:ì”¨|ë‹˜|êµìˆ˜|ë°•ì‚¬|ëŒ€í‘œ|íšŒì¥|ì‚¬ì¥|ì„ ìƒ|ë¶€ì¥|ê³¼ì¥|íŒ€ì¥))', "PERSON", 0.8),
                    # ì¡°ì§ëª… íŒ¨í„´ (ëª…í™•í•œ ì¡°ì§ ì ‘ë¯¸ì‚¬ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                    (r'[ê°€-í£A-Za-z]{2,}(?:íšŒì‚¬|ê¸°ì—…|ëŒ€í•™êµ|ëŒ€í•™|ì—°êµ¬ì†Œ|ì¬ë‹¨|í˜‘íšŒ|ì„¼í„°|ì²­|ì²˜|ë¶€ì„œ)', "ORG", 0.7),
                    # ì¥ì†Œ íŒ¨í„´ (ëª…í™•í•œ ì§€ëª… ì ‘ë¯¸ì‚¬ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                    (r'[ê°€-í£]{2,}(?:ì‹œ|ë„|êµ¬|êµ°|ë™|ì|ë©´|ë¦¬|ì—­|ê³µí•­|ë³‘ì›|í•™êµ)', "LOC", 0.7),
                    # ë‚ ì§œ íŒ¨í„´
                    (r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼|\d{1,2}ì›”\s*\d{1,2}ì¼|\d{4}ë…„', "DATE", 0.8),
                    # ìˆ«ì + ë‹¨ìœ„ (ëª…í™•í•œ ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                    (r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ì›|ë‹¬ëŸ¬|ë§Œì›|ì–µì›|ëª…|ê°œ|ì‹œê°„|ë¶„|ì´ˆ|ë¯¸í„°|í‚¬ë¡œ)', "QUANTITY", 0.7),
                    # ì œí’ˆ/ì„œë¹„ìŠ¤ëª… (ë”°ì˜´í‘œë‚˜ íŠ¹ìˆ˜ ë¬¸ìë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ê²½ìš°)
                    (r'["\'][ê°€-í£A-Za-z0-9\s]{2,}["\']', "PRODUCT", 0.6)
                ]
            else:
                # ì˜ì–´ íŒ¨í„´
                patterns = [
                    # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ (ì¸ëª…, ì¡°ì§ëª… ë“±)
                    (r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', "MISC", 0.5),
                    # ë‚ ì§œ íŒ¨í„´
                    (r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b|\b\d{4}[/-]\d{1,2}[/-]\d{1,2}\b', "DATE", 0.8),
                    # ìˆ«ì + ë‹¨ìœ„
                    (r'\b\d+(?:,\d{3})*(?:\.\d+)?\s*(?:dollars?|years?|months?|days?|hours?|minutes?)\b', "QUANTITY", 0.7)
                ]
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern, label, confidence in patterns:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    # ì¤‘ë³µ ì²´í¬
                    overlap = False
                    for existing in entities:
                        if (match.start() < existing.end and match.end() > existing.start):
                            overlap = True
                            break
                    
                    if not overlap and len(match.group().strip()) > 1:
                        entities.append(Entity(
                            id=f"rule_{label.lower()}_{len(entities)}",
                            text=match.group().strip(),
                            label=label,
                            start=match.start(),
                            end=match.end(),
                            confidence=confidence
                        ))
            
            # ì˜ë¯¸ ìˆëŠ” ì—”í‹°í‹°ë§Œ ì¶”ì¶œ (ìµœì†Œ ë³´ì¥ ë¡œì§ ì œê±°)
            logger.info(f"Rule-based extraction found {len(entities)} entities")
                        
        except Exception as e:
            logger.error(f"Rule-based entity extraction failed: {e}")
            # í´ë°± ì‹œì—ë„ ì˜ë¯¸ ìˆëŠ” ì—”í‹°í‹°ë§Œ ì¶”ì¶œ
            entities = []
        
        return entities
    
    def _deduplicate_entities(self, entities: List[Entity]) -> List[Entity]:
        """ì—”í‹°í‹° ì¤‘ë³µ ì œê±°"""
        unique_entities = []
        seen_texts = set()
        
        for entity in entities:
            if entity.text.lower() not in seen_texts:
                seen_texts.add(entity.text.lower())
                unique_entities.append(entity)
        
        return unique_entities
    
    def _deduplicate_relations(self, relations: List[Relation]) -> List[Relation]:
        """ê´€ê³„ ì¤‘ë³µ ì œê±°"""
        unique_relations = []
        seen_relations = set()
        
        for relation in relations:
            # ì†ŒìŠ¤-íƒ€ê²Ÿ-ê´€ê³„ ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            relation_key = (relation.source.lower(), relation.target.lower(), relation.relation.lower())
            if relation_key not in seen_relations:
                seen_relations.add(relation_key)
                unique_relations.append(relation)
        
        return unique_relations
    
    def _extract_relation_between_entities(self, text: str, entity1: Entity, entity2: Entity) -> Optional[Relation]:
        """ë‘ ì—”í‹°í‹° ê°„ ê´€ê³„ ì¶”ì¶œ"""
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‘ ì—”í‹°í‹°ê°€ í¬í•¨ëœ ë¬¸ë§¥ ì¶”ì¶œ
        entity1_pos = text.find(entity1.text)
        entity2_pos = text.find(entity2.text)
        
        if entity1_pos == -1 or entity2_pos == -1:
            return None
        
        # ë‘ ì—”í‹°í‹° ì‚¬ì´ì˜ ê±°ë¦¬ í™•ì¸
        distance = abs(entity1_pos - entity2_pos)
        if distance > 100:  # ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì§„ ì—”í‹°í‹°
            return None
        
        # ê´€ê³„ íŒ¨í„´ì„ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰
        relation_patterns = {
            "ì†Œì†": [
                r"([ê°€-í£A-Za-z\s]+)\s*(?:êµìˆ˜|ë°•ì‚¬|ëŒ€í‘œ|íšŒì¥|ì‚¬ì¥|ì„ ìƒ|ë¶€ì¥|ê³¼ì¥|íŒ€ì¥).*?([ê°€-í£A-Za-z]+(?:ëŒ€í•™êµ|ëŒ€í•™|íšŒì‚¬|ê¸°ì—…|ì—°êµ¬ì†Œ|ì„¼í„°))",
                r"([ê°€-í£A-Za-z\s]+).*?(?:ì—ì„œ|ì—ê²Œ|ì—)\s*(?:ì†Œì†|ê·¼ë¬´|ì¼í•˜|ì¬ì§).*?([ê°€-í£A-Za-z]+)"
            ],
            "ì—°êµ¬": [
                r"([ê°€-í£A-Za-z\s]+).*?(?:ì—ì„œ|ì—ê²Œ|ì—)\s*([ê°€-í£A-Za-z]+).*?(?:ì—°êµ¬|ê°œë°œ|ê³µë¶€)"
            ],
            "ê´€ë ¨": [
                r"([ê°€-í£A-Za-z\s]+).*?(?:ì™€|ê³¼|ì—|ì˜)\s*(?:ê´€ë ¨|ì—°ê´€|ê´€ê³„|ëŒ€í•´).*?([ê°€-í£A-Za-z]+)"
            ]
        }
        
        for relation_type, patterns in relation_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    # ë§¤ì¹˜ëœ ê·¸ë£¹ì´ ë‘ ì—”í‹°í‹°ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
                    group1, group2 = match.groups()
                    if ((entity1.text in group1 and entity2.text in group2) or 
                        (entity1.text in group2 and entity2.text in group1)):
                        return Relation(
                            source=entity1.text,  # í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                            target=entity2.text,  # í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                            relation=relation_type,
                            confidence=0.8,
                            context=match.group()
                        )
        
        # ê¸°ë³¸ íŒ¨í„´ìœ¼ë¡œ í´ë°±
        if entity1.text in text and entity2.text in text:
            return Relation(
                source=entity1.text,  # í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                target=entity2.text,  # í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                relation="ê´€ë ¨",
                confidence=0.5,
                context=f"{entity1.text}ì™€ {entity2.text}ê°€ ê°™ì€ ë¬¸ë§¥ì— ë“±ì¥"
            )
        
        return None
    
    def _extract_relations_by_patterns(self, text: str, entities: List[Entity]) -> List[Relation]:
        """íŒ¨í„´ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ"""
        relations = []
        
        # ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë§Œ ì¶”ì¶œ (ê¸°ë³¸ ì—°ê²° ì œê±°)
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities[i+1:], i+1):
                relation = self._extract_relation_between_entities(text, entity1, entity2)
                if relation:
                    relations.append(relation)
        
        return relations
    
    def _extract_relations_by_embeddings(self, text: str, entities: List[Entity]) -> List[Relation]:
        """ì„ë² ë”© ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ (GPU ê°€ì†)"""
        relations = []
        
        try:
            if not self.sentence_model or len(entities) < 2:
                return relations
            
            # GPU ì‚¬ìš© í™•ì¸
            device = next(self.sentence_model.parameters()).device
            logger.info(f"Embedding extraction using device: {device}")
            
            # ì—”í‹°í‹° ìŒë³„ ìœ ì‚¬ë„ ê³„ì‚° (GPUì—ì„œ ìˆ˜í–‰)
            entity_texts = [e.text for e in entities]
            embeddings = self.sentence_model.encode(
                entity_texts,
                convert_to_tensor=True,  # GPU tensorë¡œ ë³€í™˜
                show_progress_bar=False
            )
            
            # GPUì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
            import torch
            if torch.cuda.is_available() and device.type == 'cuda':
                embeddings = embeddings.to(device)
            
            for i, entity1 in enumerate(entities):
                for j, entity2 in enumerate(entities[i+1:], i+1):
                    # GPUì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = torch.cosine_similarity(
                        embeddings[i].unsqueeze(0), 
                        embeddings[j].unsqueeze(0)
                    ).item()
                    
                    if similarity > 0.7:  # ë†’ì€ ìœ ì‚¬ë„
                        relations.append(Relation(
                            source=entity1.text,  # í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                            target=entity2.text,  # í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ì‚¬ìš©
                            relation="ìœ ì‚¬",
                            confidence=float(similarity),
                            context=f"ì„ë² ë”© ìœ ì‚¬ë„ (GPU ê°€ì†, ìœ ì‚¬ë„: {similarity:.3f})"
                        ))
        
        except Exception as e:
            logger.error(f"Embedding-based relation extraction failed: {e}")
        
        return relations
    
    def _extract_relations_fallback(self, entities: List[Entity]) -> List[Relation]:
        """í´ë°± ê´€ê³„ ì¶”ì¶œ - ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë§Œ"""
        relations = []
        
        # í´ë°±ì—ì„œë„ ë¬´ì˜ë¯¸í•œ ê´€ê³„ ìƒì„±í•˜ì§€ ì•ŠìŒ
        logger.info("Using fallback relation extraction - no automatic relations")
        
        return relations
    
    def _extract_date_attributes(self, text: str, entity: Entity) -> Dict[str, Any]:
        """ë‚ ì§œ ì†ì„± ì¶”ì¶œ"""
        attributes = {}
        
        # ì—”í‹°í‹° ì£¼ë³€ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        context_start = max(0, entity.start - 50)
        context_end = min(len(text), entity.end + 50)
        context = text[context_start:context_end]
        
        date_patterns = [
            r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼',
            r'\d{4}-\d{2}-\d{2}',
            r'\d{1,2}/\d{1,2}/\d{4}'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, context)
            if matches:
                attributes['date'] = matches[0]
                break
        
        return attributes
    
    def _extract_numeric_attributes(self, text: str, entity: Entity) -> Dict[str, Any]:
        """ìˆ˜ì¹˜ ì†ì„± ì¶”ì¶œ"""
        attributes = {}
        
        context_start = max(0, entity.start - 30)
        context_end = min(len(text), entity.end + 30)
        context = text[context_start:context_end]
        
        # ìˆ«ì + ë‹¨ìœ„ íŒ¨í„´
        numeric_patterns = [
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ì›|ë‹¬ëŸ¬|ëª…|ê°œ|ë…„|ì›”|ì¼|ì‹œê°„|ë¶„|ì´ˆ|ë¯¸í„°|í‚¬ë¡œ|í†¤)',
            r'\d+(?:,\d{3})*(?:\.\d+)?%',
            r'\d+(?:,\d{3})*(?:\.\d+)?'
        ]
        
        for pattern in numeric_patterns:
            matches = re.findall(pattern, context)
            if matches:
                attributes['numeric_value'] = matches[0]
                break
        
        return attributes
    
    def _extract_location_attributes(self, text: str, entity: Entity) -> Dict[str, Any]:
        """ìœ„ì¹˜ ì†ì„± ì¶”ì¶œ"""
        attributes = {}
        
        if entity.label in ["LOC", "GPE", "ìœ„ì¹˜", "ì¥ì†Œ"]:
            attributes['location_type'] = entity.label
            
            # ì£¼ì†Œ íŒ¨í„´
            context_start = max(0, entity.start - 50)
            context_end = min(len(text), entity.end + 50)
            context = text[context_start:context_end]
            
            address_pattern = r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°|ë™|ì|ë©´|ë¦¬)\s*[ê°€-í£0-9\-\s]*'
            matches = re.findall(address_pattern, context)
            if matches:
                attributes['address'] = matches[0]
        
        return attributes
    
    def _extract_category_attributes(self, text: str, entity: Entity) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ ì†ì„± ì¶”ì¶œ"""
        attributes = {}
        
        # ì—”í‹°í‹° íƒ€ì…ë³„ ì¹´í…Œê³ ë¦¬
        category_mapping = {
            "PERSON": "ì¸ë¬¼",
            "ORG": "ì¡°ì§",
            "LOC": "ìœ„ì¹˜",
            "DATE": "ë‚ ì§œ",
            "MONEY": "ê¸ˆì•¡",
            "PERCENT": "ë¹„ìœ¨"
        }
        
        if entity.label in category_mapping:
            attributes['category'] = category_mapping[entity.label]
        
        return attributes
    
    def _entity_to_dict(self, entity: Entity) -> Dict[str, Any]:
        """ì—”í‹°í‹°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "id": entity.id,
            "text": entity.text,
            "label": entity.label,
            "start": entity.start,
            "end": entity.end,
            "confidence": entity.confidence,
            "attributes": entity.attributes
        }
    
    def _relation_to_dict(self, relation: Relation) -> Dict[str, Any]:
        """ê´€ê³„ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "source": relation.source,
            "target": relation.target,
            "relation": relation.relation,
            "confidence": relation.confidence,
            "context": relation.context,
            "attributes": relation.attributes
        }
    
    def _get_processing_stats(self, entities: List[Entity], relations: List[Relation]) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„"""
        return {
            "total_entities": len(entities),
            "total_relations": len(relations),
            "entity_types": list(set([e.label for e in entities])),
            "relation_types": list(set([r.relation for r in relations])),
            "avg_entity_confidence": np.mean([e.confidence for e in entities]) if entities else 0,
            "avg_relation_confidence": np.mean([r.confidence for r in relations]) if relations else 0
        }
    
    def _fallback_processing(self, text: str) -> Dict[str, Any]:
        """í´ë°± ì²˜ë¦¬"""
        logger.warning("Using fallback processing")
        
        # ê¸°ë³¸ ì—”í‹°í‹° ì¶”ì¶œ
        words = text.split()
        entities = []
        for i, word in enumerate(words[:5]):  # ì²˜ìŒ 5ê°œ ë‹¨ì–´ë¥¼ ì—”í‹°í‹°ë¡œ
            entities.append({
                "id": f"fallback_entity_{i}",
                "text": word,
                "label": "MISC",
                "start": 0,
                "end": len(word),
                "confidence": 0.3,
                "attributes": {}
            })
        
        # ê¸°ë³¸ ê´€ê³„
        relations = []
        if len(entities) >= 2:
            relations.append({
                "source": entities[0]["id"],
                "target": entities[1]["id"],
                "relation": "ê´€ë ¨",
                "confidence": 0.3,
                "context": "í´ë°± ì²˜ë¦¬",
                "attributes": {}
            })
        
        return {
            "original_text": text,
            "preprocessed_text": text,
            "entities": entities,
            "relations": relations,
            "graph": {
                "nodes": [
                    {
                        **e,
                        "node_type": e["label"],
                        "x": np.random.randint(100, 700),
                        "y": np.random.randint(100, 600)
                    } for e in entities
                ],
                "edges": [
                    {
                        "id": f"edge_{i}",
                        **r
                    } for i, r in enumerate(relations)
                ],
                "summary": {
                    "node_count": len(entities),
                    "edge_count": len(relations),
                    "node_types": ["MISC"],
                    "processing_language": self.language,
                    "confidence_avg": 0.3
                }
            },
            "processing_stats": {
                "total_entities": len(entities),
                "total_relations": len(relations),
                "entity_types": ["MISC"],
                "relation_types": ["ê´€ë ¨"],
                "avg_entity_confidence": 0.3,
                "avg_relation_confidence": 0.3
            }
        } 